---
layout: post
title: "[ML] Decision Tree"
date: 2024-09-10 12:00:00 +0900
categories: Machine_Learning
use_math: true
---

### **Decision Tree (의사결정나무)**
**트리 구조**를 사용하여 데이터를 분류하거나 예측하는 머신러닝 알고리즘<br>
- **쉽게 해석 가능**하고 **직관적** <br>
- **분류(Classification)**와 **회귀(Regression)** 문제 모두에서 사용할 수 있다. 
<br><br>


1. **Root Node**
: 트리의 최상단 노드로, 전체 데이터를 포함한다. 이 노드에서부터 데이터가 분기된다.

2. **Internal Node**
: 각 노드는 특정한 **Feature**에 대한 조건을 가지고 데이터를 나눈다.

3. **Leaf Node**
: 최종적인 분기 끝에 위치한 노드로, 더 이상 데이터를 나누지 않고 **최종 예측 결과**를 나타낸다.

4. **Branch**
: 노드 간의 경로를 나타내며, 트리의 각 경로는 **특정한 규칙**을 기반으로 데이터를 분류하는 과정을 나타낸다.

### Decision Tree 작동 방식

1. **특징 선택**
: 트리의 각 분기에서 어떤 특징을 기준으로 데이터를 나눌지 결정해야 한다. 일반적으로 **Information Gain(정보이득)**이나 **Gini Index(지니지수)**와 같은 기준을 사용하여 분할한다.

2. **데이터 분할**
: 각 노드는 특정 특징을 기준으로 데이터를 두 개 이상의 그룹으로 나눈다. 이 과정을 트리의 리프 노드까지 진행하며, 리프 노드에 도달하면 분류가 완료된다.

3. **멈춤 기준**
: 더 이상 분할할 수 없거나 분할이 의미 없을 때 트리의 성장을 멈춥니다. 이를 위해 조건을 설정할 수 있는데, 예를 들어, 더 이상 분할이 유의미하지 않거나 리프 노드에 속한 데이터 수가 특정 임계값 이하일 때 멈춥니다.

> **Information Gain**
>: 데이터의 불확실성을 줄이는 정도<br>이를 측정하는 방식은 **Entropy**라는 개념을 사용하는데, 엔트로피는 데이터의 혼잡도(불확실성)를 나타내며, 더 균일할수록 낮고, 혼합될수록 높다. 
>
>   Information Gain = 부모 노드의 엔트로피 -- 자식 노드의 가중치 엔트로피 합

<br>

> **Gini Index**
>: 한 그룹의 순수도를 측정하는 지표 <br>그룹 내 데이터가 한 클래스에 속할수록 낮아진다.<br> Gini Index 가 낮을수록 해당 노드는 순수하다는 의미.
>   <br><br>
>   $G = 1 - \sum_{i=1}^{C} p_i^2$
>   - $G$ : Gini Index
>   - $C$ : 클래스의 개수 (예: 이진 분류면 \( $C = 2$ \), 다중 분류일 경우 \( $C$ \)는 클래스의 수)
>   - $p_i$ : 해당 클래스에 속할 확률, 즉 해당 클래스의 비율
>
>   Gini Index 는 **0**에서 **1** 사이의 값을 가진다.
>   <br>
>   
>   **예시**
>   : 만약 A 70개, B 30개라면
>   - A의 비율 \( $p_{\text{A}} = 0.7$ \)
>   - B의 비율 \( $p_{\text{B}} = 0.3$ \)
>   
>   $G = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 0.42$
>   
>   Gini Index = 0.42로, 두 클래스가 부분적으로 혼합되어 있음을 나타낸다.


### Decision Tree 의 장점

1. **해석 용이**:
   의사결정나무는 시각적으로 표현하기 쉬워 모델을 해석하기가 매우 용이하다. 규칙 기반이기 때문에 어떤 이유로 특정 결과가 도출되었는지 쉽게 알 수 있다.

2. **비선형 데이터 처리**:
   비선형 관계를 가진 데이터를 처리하는 데 효과적이다. 트리의 분기 구조가 복잡한 의사결정을 잘 모델링한다.

3. **전처리 과정이 적음**:
   특성 스케일링(정규화, 표준화)이나 특성 간 상관 관계 제거 등의 전처리 과정이 거의 필요하지 않다.

### Decision Tree 의 단점

1. **Overfitting(과적합)**:
   트리가 너무 깊어지면 학습 데이터에 지나치게 맞추는 현상이 발생할 수 있다. 이는 일반화 성능을 떨어뜨리기 때문에, **Pruning(가지치기)**를 통해 트리의 크기를 제어해야 한다.

2. **불안정성**:
   의사결정나무는 작은 데이터 변화에도 큰 영향을 받을 수 있다. 트리의 구조가 약간의 데이터 차이로 완전히 바뀔 수 있기 때문에 **Random Forest**와 같은 앙상블 기법이 자주 사용된다.

<br>

> **Pruning** <br>
트리가 과적합되지 않도록 **불필요한 노드**를 제거하는 과정 <br> **Pre-Pruning**와 **Post-Pruning** 방식으로 나뉘며, Pre-Pruning 은 트리가 너무 커지기 전에 성장 제한을 두고, Post-Pruning 은 트리가 완성된 후 성능을 보며 노드를 제거합니다.

<br>

---------
<br>

## **Decision Tree Regression**
: **연속적인 값** 을 예측하는 데 사용되는 머신러닝 모델 <br> 
Decision Tree Regression 는 데이터를 **특성**을 기준으로 **재귀적으로 분할**하면서, 각 분할에 대해 **평균 값**을 계산하여 최종 예측을 수행한다.

### Decision Tree Regression 동작 방식

1. **데이터 분할**:
   의사결정나무 회귀는 데이터가 속한 특성을 기준으로 **최적의 분할 지점**을 찾는다. 각 노드는 데이터를 여러 부분으로 나누며, 분할 기준은 예측하려는 **목표 값**의 분산을 최소화하는 방향으로 결정된다.

2. **각 분할에서 평균 값 계산**:
   데이터를 분할한 후, leaf node 에 도달하게 되면 그 노드에 속한 데이터들의 **평균**을 사용하여 예측 값을 결정한다. 즉, leaf node 에서는 그 구간에 속한 값들의 평균을 예측 값으로 반환한다.

3. **순차적인 분할**:
   각 단계에서 데이터를 분할할 때, 모델은 분산이 가장 많이 감소하는 지점을 찾는다. 목표는 데이터의 **Variance(분산)**을 최소화하는 방향으로 나누는 것이다. 이를 통해 최종적으로 **leaf node**가 결정되고, 해당 노드에 포함된 데이터의 평균이 예측 값이 된다.


### Decision Tree Regression 손실 함수

**분류**에서는 노드의 순수도를 평가할 때 **Gini Index**나 **Entropy**와 같은 지표를 사용하지만, **회귀**에서는 분할된 노드 내에서 목표 값의 **Variance**을 평가합니다.

- **Mean Squared Error, MSE(평균 제곱 오차)**:
  Decision Tree Regression 에서 가장 많이 사용되는 손실 함수는 **MSE**이다. MSE는 예측 값과 실제 값 사이의 오차를 제곱하여 평균을 구한 값. 각 분할에서 **MSE가 최소화**되는 방향으로 데이터를 분할한다.


   $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y})^2$
   - $y_i$ : 실제 값
   - $\hat{y}$ : 예측 값
   - $n$ : 데이터 포인트 수

### 의사결정나무 회귀와 분류의 차이점

- **출력 값**:
  - **분류**: 예측 값이 **카테고리(클래스)**
  - **회귀**: 예측 값이 **연속적인 실수**
  
- **손실 함수**:
  - **분류**: Gini Index, Entropy 등을 사용하여 분류 기준을 설정
  - **회귀**: MSE나 분산과 같은 기준을 사용하여 분할