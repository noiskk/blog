---
layout: post
title: "CH3. Computer Abstractions and Technology"
date: 2024-09-27 12:00:00 +0900
categories: Computer_Architecture
---
## Instruction Set : Language of the Computer

**Classification**(분류)은 주어진 데이터로부터 모델을 학습하여 입력 데이터를 미리 정의된 **카테고리**나 **레이블** 중 하나로 분류하는 작업을 말합니다. 즉, 특정 데이터가 어느 범주에 속하는지 예측하는 것이 분류의 목표입니다.

  

**Classification의 주요 개념**

  

1. **레이블(Label)**:

분류 모델이 예측할 **타겟 값**입니다. 예를 들어, 이메일을 스팸/정상으로 분류한다면 스팸과 정상은 레이블이 됩니다.

2. **클래스(Class)**:

분류의 **범주**를 의미합니다. 이진 분류(binary classification)에서는 클래스가 2개 (예: 참/거짓, 스팸/정상)이고, 다중 클래스 분류(multi-class classification)에서는 클래스가 3개 이상입니다.

3. **특징(Feature)**:

분류할 때 모델이 활용하는 **입력 데이터**의 속성입니다. 예를 들어, 이메일 분류에서는 이메일 본문 내용, 발신자 정보, 제목 등이 특징이 될 수 있습니다.

  

**주요 분류 알고리즘**

  

1. **로지스틱 회귀(Logistic Regression)**:

선형 회귀와 유사하지만 출력이 확률로 나타나며, 그 확률을 바탕으로 이진 분류를 수행합니다. S자 모양의 시그모이드 함수로 분류를 진행합니다.

2. **K-최근접 이웃(K-Nearest Neighbors, KNN)**:

새로운 데이터를 분류할 때, 가장 가까운 K개의 이웃 데이터를 참고하여 분류합니다. 데이터가 분포된 공간에서 가까운 이웃의 다수결을 따르는 방식입니다.

3. **서포트 벡터 머신(SVM)**:

데이터를 분류할 수 있는 최적의 **결정 경계(하이퍼플레인)**를 찾습니다. 결정 경계와 가장 가까운 데이터를 고려하여 분류를 수행합니다.

4. **의사결정나무(Decision Tree)**:

데이터의 특징을 기준으로 분기해 나가면서 최종적으로 분류하는 방식입니다. 매우 직관적이어서 해석이 용이하지만, 과적합(overfitting)의 위험이 있습니다.

5. **랜덤 포레스트(Random Forest)**:

여러 개의 의사결정나무를 사용하여 예측을 수행하고, 그 예측을 종합하여 최종 분류 결과를 도출하는 앙상블 학습 방법입니다.

6. **나이브 베이즈(Naive Bayes)**:

각 특징이 독립적이라고 가정하고 베이즈 정리를 사용하여 확률 기반으로 분류를 진행합니다. 텍스트 분류에서 자주 사용됩니다.

7. **인공신경망(Artificial Neural Network)**:

인간의 뇌 구조를 모방한 모델로, 여러 계층을 거쳐 데이터를 처리하며 학습을 진행합니다. 특히 **딥러닝**에서는 이 방법이 주로 사용됩니다.

  

**Classification의 종류**

  

1. **이진 분류(Binary Classification)**:

두 가지 클래스 중 하나로 분류하는 문제입니다. 예를 들어, 환자가 질병이 있는지 여부(Yes/No), 이메일이 스팸인지 여부(스팸/정상) 등이 이에 해당합니다.

2. **다중 클래스 분류(Multi-Class Classification)**:

세 개 이상의 클래스 중 하나로 분류하는 문제입니다. 예를 들어, 이미지 분류에서 개, 고양이, 새와 같은 여러 클래스로 분류하는 경우입니다.

3. **다중 레이블 분류(Multi-Label Classification)**:

하나의 데이터가 여러 개의 클래스에 속할 수 있는 문제입니다. 예를 들어, 한 영화가 액션, 코미디 두 장르에 속할 수 있습니다.

  

**분류 모델의 성능 평가**

  

1. **정확도(Accuracy)**:

전체 예측에서 맞춘 비율입니다. 그러나 클래스 불균형이 심한 경우에는 신뢰성이 낮을 수 있습니다.

2. **정밀도(Precision)**:

모델이 양성(Positive)으로 예측한 것들 중 실제로 맞는 비율입니다.

3. **재현율(Recall)**:

실제 양성 중에서 모델이 양성으로 예측한 비율입니다. 재현율은 중요도가 높은 경우(예: 질병 진단)에서 중요합니다.

4. **F1 Score**:

정밀도와 재현율의 조화평균입니다. 정밀도와 재현율 간 균형이 중요할 때 유용한 지표입니다.

5. **AUC-ROC**:

분류기의 성능을 평가하는 그래프로, ROC 곡선 아래 면적(AUC)으로 모델의 분류 성능을 평가합니다. 1에 가까울수록 좋은 성능을 나타냅니다.

  

분류 문제는 머신러닝에서 매우 중요한 역할을 하며, 데이터의 특성과 문제의 목적에 맞는 알고리즘을 선택하는 것이 중요합니다.


------------


**의사결정나무(Decision Tree)**는 **트리 구조**를 사용하여 데이터를 분류하거나 예측하는 머신러닝 알고리즘입니다. 이 모델은 **쉽게 해석 가능**하고 **직관적**이라서 자주 사용됩니다. 특히 **분류(Classification)**와 **회귀(Regression)** 문제 모두에서 사용할 수 있습니다.

### 의사결정나무의 기본 개념

1. **루트 노드(Root Node)**:
   트리의 최상단 노드로, 전체 데이터를 포함하고 있습니다. 이 노드에서부터 데이터가 분기됩니다.

2. **내부 노드(Internal Node)**:
   각 노드는 특정한 **특징(Feature)**에 대한 조건을 가지고 데이터를 나눕니다. 예를 들어, `Temperature > 30도인가?`라는 조건에 따라 데이터를 두 그룹으로 나눌 수 있습니다.

3. **리프 노드(Leaf Node)**:
   최종적인 분기 끝에 위치한 노드로, 더 이상 데이터를 나누지 않고 **최종 예측 결과**를 나타냅니다. 예를 들어, 분류 문제라면 리프 노드는 특정 클래스(예: 스팸/정상)를 나타냅니다.

4. **가지(Branch)**:
   노드 간의 경로를 나타내며, 트리의 각 경로는 **특정한 규칙**을 기반으로 데이터를 분류하는 과정을 나타냅니다.

### 의사결정나무의 작동 방식

1. **특징 선택**:
   트리의 각 분기에서 어떤 특징을 기준으로 데이터를 나눌지 결정해야 합니다. 일반적으로 **정보 이득(Information Gain)**이나 **지니 지수(Gini Index)**와 같은 기준을 사용하여 분할합니다.

2. **데이터 분할**:
   각 노드는 특정 특징을 기준으로 데이터를 두 개 이상의 그룹으로 나눕니다. 이 과정을 트리의 리프 노드까지 진행하며, 리프 노드에 도달하면 분류가 완료됩니다.

3. **멈춤 기준**:
   더 이상 분할할 수 없거나 분할이 의미 없을 때 트리의 성장을 멈춥니다. 이를 위해 조건을 설정할 수 있는데, 예를 들어, 더 이상 분할이 유의미하지 않거나 리프 노드에 속한 데이터 수가 특정 임계값 이하일 때 멈춥니다.

### 특징 선택 기준

1. **정보 이득(Information Gain)**:
   정보 이득은 데이터의 불확실성을 줄이는 정도를 나타냅니다. 이를 측정하는 방식은 **엔트로피(Entropy)**라는 개념을 사용하는데, 엔트로피는 데이터의 혼잡도(불확실성)를 나타내며, 더 균일할수록 낮고, 혼합될수록 높습니다. 
   
   정보 이득 = 부모 노드의 엔트로피 - 자식 노드의 가중치 엔트로피 합

2. **지니 지수(Gini Index)**:
   한 그룹의 순수도를 측정하는 지표입니다. 그룹 내 데이터가 한 클래스에 속할수록 지니 지수가 낮아집니다. 지니 지수가 낮을수록 해당 노드는 순수하다고 할 수 있습니다.
   
   지니 지수 = 1 - ∑(각 클래스의 비율^2)

### 장점

1. **해석 용이**:
   의사결정나무는 시각적으로 표현하기 쉬워 모델을 해석하기가 매우 용이합니다. 규칙 기반이기 때문에 어떤 이유로 특정 결과가 도출되었는지 쉽게 알 수 있습니다.

2. **비선형 데이터 처리**:
   비선형 관계를 가진 데이터를 처리하는 데 효과적입니다. 트리의 분기 구조가 복잡한 의사결정을 잘 모델링합니다.

3. **전처리 과정이 적음**:
   특성 스케일링(정규화, 표준화)이나 특성 간 상관 관계 제거 등의 전처리 과정이 거의 필요하지 않습니다.

### 단점

1. **과적합(Overfitting)**:
   트리가 너무 깊어지면 학습 데이터에 지나치게 맞추는 현상이 발생할 수 있습니다. 이는 일반화 성능을 떨어뜨리기 때문에, **가지치기(Pruning)**를 통해 트리의 크기를 제어해야 합니다.

2. **불안정성**:
   의사결정나무는 작은 데이터 변화에도 큰 영향을 받을 수 있습니다. 트리의 구조가 약간의 데이터 차이로 완전히 바뀔 수 있기 때문에 **랜덤 포레스트(Random Forest)**와 같은 앙상블 기법이 자주 사용됩니다.

### 가지치기(Pruning)

가지치기는 트리가 과적합되지 않도록 **불필요한 노드**를 제거하는 과정입니다. **사전 가지치기(Pre-Pruning)**와 **사후 가지치기(Post-Pruning)** 방식으로 나뉘며, 사전 가지치기는 트리가 너무 커지기 전에 성장 제한을 두고, 사후 가지치기는 트리가 완성된 후 성능을 보며 노드를 제거합니다.

### 의사결정나무의 활용

1. **분류 문제**:
   - 이메일 분류(스팸/정상)
   - 암 진단(악성/양성)

2. **회귀 문제**:
   - 주택 가격 예측
   - 판매량 예측

의사결정나무는 다양한 분야에서 **간단하고 효과적인 모델**로 널리 사용되며, 특히 직관적 해석이 필요한 상황에서 유용합니다.


--------------

**지니 지수(Gini Index)**는 **의사결정나무(Decision Tree)**에서 노드를 분할할 때 각 분할의 **순수도(purity)**를 측정하는 데 사용되는 지표입니다. 순수도란, 데이터를 분류할 때 그룹 내에 있는 데이터가 얼마나 **같은 클래스**에 속해 있는지를 나타내는 개념입니다. 지니 지수가 낮을수록 해당 그룹이 **순수**하다는 것을 의미하며, 지니 지수가 높을수록 해당 그룹이 **혼합**되어 있다는 것을 의미합니다.

### 지니 지수의 수식

지니 지수는 다음과 같은 수식으로 계산됩니다:

\[
G = 1 - \sum_{i=1}^{C} p_i^2
\]

여기서:
- \( G \): 지니 지수(Gini Index)
- \( C \): 클래스의 개수 (예: 이진 분류면 \( C = 2 \), 다중 분류일 경우 \( C \)는 클래스의 수)
- \( p_i \): 해당 클래스에 속할 확률, 즉 해당 클래스의 비율

지니 지수는 **0**에서 **1** 사이의 값을 가지며, 값이 낮을수록 노드 내에서 데이터가 동일한 클래스에 속할 확률이 높다는 것을 나타냅니다.

### 예시

#### 이진 분류(Binary Classification) 예시:
만약 한 노드에 **스팸(Spam)**과 **정상(Ham)** 두 가지 클래스가 있다고 가정해보겠습니다.

1. **첫 번째 경우**: 
   노드에 스팸 100개, 정상 0개가 있다면, 즉 노드가 전부 스팸으로만 이루어져 있다면:
   
   - 스팸의 비율 \( p_{\text{spam}} = 1 \)
   - 정상의 비율 \( p_{\text{ham}} = 0 \)
   
   \[
   G = 1 - (1^2 + 0^2) = 1 - 1 = 0
   \]
   
   지니 지수는 0으로, **이 노드는 완전히 순수**하다는 것을 의미합니다.

2. **두 번째 경우**:
   만약 스팸 50개, 정상 50개가 있다면, 즉 노드가 두 클래스가 동일하게 섞여 있다면:
   
   - 스팸의 비율 \( p_{\text{spam}} = 0.5 \)
   - 정상의 비율 \( p_{\text{ham}} = 0.5 \)
   
   \[
   G = 1 - (0.5^2 + 0.5^2) = 1 - (0.25 + 0.25) = 0.5
   \]
   
   지니 지수가 0.5로, **완전히 혼합된 상태**를 나타냅니다.

3. **세 번째 경우**:
   만약 스팸 70개, 정상 30개라면:
   
   - 스팸의 비율 \( p_{\text{spam}} = 0.7 \)
   - 정상의 비율 \( p_{\text{ham}} = 0.3 \)
   
   \[
   G = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 0.42
   \]
   
   지니 지수가 0.42로, 두 클래스가 부분적으로 혼합되어 있음을 나타냅니다.

### 지니 지수의 특징

1. **값 범위**:
   - 지니 지수는 0에서 0.5 사이에서 작동합니다.
   - 이진 분류에서는 **0**일 때 가장 순수하고, **0.5**일 때 가장 혼합된 상태입니다.
   - 다중 클래스에서는 혼합된 클래스가 많아질수록 지니 지수가 최대 **1**에 가까워집니다.

2. **노드 분할**:
   의사결정나무는 각 노드를 분할할 때 지니 지수가 가장 **낮아지도록** 분할하는 기준을 선택합니다. 즉, **지니 지수를 최소화하는 방향**으로 트리를 성장시켜 노드가 가능한 한 순수해지도록 만듭니다.

3. **해석의 용이성**:
   지니 지수는 계산이 간단하고, 직관적으로 **분포의 불균형**을 파악할 수 있습니다. 클래스 비율에 따라 노드의 순수도를 평가하는 방식이므로, 클래스 간 불균형이 큰 경우에도 유용하게 사용됩니다.

### 지니 지수와 엔트로피 비교

의사결정나무에서 노드를 분할할 때 **엔트로피(Entropy)**를 사용하는 경우도 많습니다. 엔트로피는 정보 이론에 기반을 둔 지표로, 클래스의 불확실성을 측정합니다.

#### 공통점:
- 둘 다 노드의 **순수도**를 측정하는 지표입니다.
- 노드를 **최대한 순수하게** 분할하기 위한 기준으로 사용됩니다.

#### 차이점:
- **계산 방식**: 엔트로피는 로그 함수를 사용하여 복잡도가 조금 더 높지만, 지니 지수는 **비율의 제곱**으로 계산되기 때문에 더 단순합니다.
- **결과 값**: 두 지표 모두 결과는 비슷하게 나타나지만, 지니 지수가 일반적으로 **좀 더 빠르게** 계산되며, 다소 **과감하게** 분할하는 경향이 있습니다.

### 정리

- 지니 지수는 의사결정나무에서 **데이터 순수도를 측정**하는 중요한 지표로 사용되며, 분할된 노드가 얼마나 순수한지(동일한 클래스인지)를 평가합니다.
- 계산이 간단하고 직관적이며, 엔트로피에 비해 **계산 효율성**이 좋습니다.
- 트리 구조에서 데이터가 잘 섞여 있는지, 아니면 특정 클래스에 치우쳐 있는지를 파악할 수 있습니다.


---------

**Decision Tree Regression**(의사결정나무 회귀)은 **연속적인 값**을 예측하는 데 사용되는 머신러닝 모델입니다. 이는 의사결정나무가 분류 문제뿐만 아니라 회귀 문제에도 적용될 수 있음을 보여줍니다. 의사결정나무 회귀는 데이터를 **특성**을 기준으로 **재귀적으로 분할**하면서, 각 분할에 대해 **평균 값**을 계산하여 최종 예측을 수행합니다.

### Decision Tree Regression의 동작 방식

1. **데이터 분할**:
   의사결정나무 회귀는 데이터가 속한 특성을 기준으로 **최적의 분할 지점**을 찾습니다. 각 노드는 데이터를 여러 부분으로 나누며, 분할 기준은 예측하려는 **목표 값**의 분산을 최소화하는 방향으로 결정됩니다.

2. **각 분할에서 평균 값 계산**:
   데이터를 분할한 후, 리프 노드에 도달하게 되면 그 노드에 속한 데이터들의 **평균**을 사용하여 예측 값을 결정합니다. 즉, 리프 노드에서는 그 구간에 속한 값들의 평균을 예측 값으로 반환합니다.

3. **순차적인 분할**:
   각 단계에서 데이터를 분할할 때, 모델은 분산이 가장 많이 감소하는 지점을 찾습니다. 목표는 데이터의 **분산(Variance)**을 최소화하는 방향으로 나누는 것입니다. 이를 통해 최종적으로 **리프 노드**가 결정되고, 해당 노드에 포함된 데이터의 평균이 예측 값이 됩니다.

### 예시

#### 간단한 예제
예를 들어, 특정 집의 가격을 예측하는 모델을 만들 때, 의사결정나무 회귀는 집의 크기, 방의 수, 위치와 같은 특징을 기준으로 데이터를 분할합니다. 그리고 각각의 분할에 도달한 후, 그 그룹에 속하는 집들의 평균 가격을 예측 값으로 제시합니다.

1. 첫 번째 분할: 집의 크기(평수)에 따라 데이터를 두 그룹으로 나눔.
2. 두 번째 분할: 각 그룹 내에서 방의 수에 따라 다시 세분화.
3. 리프 노드에서는 분할된 데이터의 집 가격의 평균을 예측 값으로 사용.

### 의사결정나무 회귀에서의 손실 함수

**분류**에서는 노드의 순수도를 평가할 때 **지니 지수**나 **엔트로피**와 같은 지표를 사용하지만, **회귀**에서는 분할된 노드 내에서 목표 값의 **분산(Variance)**을 평가합니다.

- **평균 제곱 오차(Mean Squared Error, MSE)**:
  의사결정나무 회귀에서 가장 많이 사용되는 손실 함수는 **MSE**입니다. MSE는 예측 값과 실제 값 사이의 오차를 제곱하여 평균을 구한 값입니다. 각 분할에서 **MSE가 최소화**되는 방향으로 데이터를 분할합니다.

\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y})^2
\]

여기서:
- \( y_i \): 실제 값
- \( \hat{y} \): 예측 값
- \( n \): 데이터 포인트 수

### 가지치기(Pruning)

의사결정나무 회귀는 데이터를 너무 과하게 세분화할 경우 **과적합(Overfitting)** 문제가 발생할 수 있습니다. 이를 방지하기 위해 **가지치기(Pruning)**를 사용하여 트리의 크기를 제어합니다. 가지치기는 트리가 너무 복잡해지지 않도록 **불필요한 노드**를 제거하여, 모델이 학습 데이터에 지나치게 맞추지 않게 도와줍니다.

- **사전 가지치기(Pre-Pruning)**: 트리를 구축하는 동안 일정 깊이 이상으로 성장하지 않도록 제약을 걸거나, 리프 노드에 남은 데이터 포인트 수가 일정 수 이하일 경우 더 이상 분할하지 않습니다.
- **사후 가지치기(Post-Pruning)**: 트리 전체가 완성된 후, 성능을 향상시키기 위해 불필요한 노드를 제거합니다.

### 장점

1. **해석 용이**: 의사결정나무는 직관적이고 시각화가 가능하여 **결과 해석이 용이**합니다.
2. **비선형 데이터 처리**: 비선형적인 데이터 관계를 잘 처리할 수 있습니다.
3. **전처리 과정이 적음**: 데이터 스케일링이나 특성 간 상관관계를 크게 고려하지 않고도 사용할 수 있습니다.
4. **모든 범주의 데이터 처리**: 연속형 데이터와 범주형 데이터를 모두 처리할 수 있습니다.

### 단점

1. **과적합 위험**: 트리가 깊어지면 학습 데이터에 너무 맞추어 **과적합**이 발생할 수 있습니다. 이를 방지하기 위해 가지치기를 사용해야 합니다.
2. **불안정성**: 데이터의 작은 변화에도 트리의 구조가 크게 변할 수 있습니다.
3. **최적의 분할을 찾기 위한 비용**: 최적의 분할을 찾기 위한 계산 비용이 커질 수 있습니다. 따라서 데이터가 많아질수록 트리의 학습 시간이 길어집니다.

### 의사결정나무 회귀와 분류의 차이점

- **출력 값**:
  - **분류**: 예측 값이 **카테고리(클래스)**에 해당합니다.
  - **회귀**: 예측 값이 **연속적인 실수**입니다.
  
- **손실 함수**:
  - **분류**: 지니 지수, 엔트로피 등을 사용하여 분류 기준을 설정합니다.
  - **회귀**: MSE나 분산과 같은 기준을 사용하여 분할합니다.

### 의사결정나무 회귀의 활용 예시

1. **주택 가격 예측**: 주택의 크기, 위치, 방의 개수 등을 기반으로 특정 주택의 가격을 예측합니다.
2. **주식 가격 예측**: 과거 데이터를 기반으로 미래 주식 가격을 예측합니다.
3. **비용 예측**: 고객의 소비 습관을 바탕으로 향후 소비 패턴을 예측하거나, 특정 서비스에 대한 비용을 예측하는 데 사용할 수 있습니다.

### 결론

**Decision Tree Regression**은 **직관적**이고, **비선형 관계**를 잘 처리할 수 있어 다양한 회귀 문제에서 유용하게 활용됩니다. 하지만 과적합에 주의해야 하며, 가지치기를 통해 모델의 복잡도를 적절히 제어해야 합니다.