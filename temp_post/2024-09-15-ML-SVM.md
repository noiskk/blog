---
layout: post
title: "[ML] SVM(Support Vector Machine)"
date: 2024-09-15 12:00:00 +0900
categories: Machine_Learning
use_math: true
---

### **SVM, Support Vector Machine**
: 분류(Classification)와 회귀(Regression) 작업에 모두 사용할 수 있는 강력한 지도 학습 알고리즘<br>. 특히 **분류 문제**에서 매우 효과적으로 작동하며, **고차원 공간**에서 데이터 포인트를 분리하는 최적의 결정 경계를 찾는 방식으로 작동합니다.

### SVM의 주요 개념

1. **초평면(Hyperplane)**:
   SVM은 입력 데이터 공간에서 **초평면**을 사용하여 두 개의 클래스를 구분합니다. 초평면은 고차원 공간에서 데이터를 나누는 경계를 의미하며, 2차원에서는 선, 3차원에서는 평면, 그 이상의 차원에서는 초평면이라고 부릅니다.

2. **서포트 벡터(Support Vectors)**:
   서포트 벡터는 **결정 경계(초평면)**에 가장 가까이 위치한 데이터 포인트들을 의미합니다. SVM은 이러한 서포트 벡터들에 의해 초평면을 결정하며, 나머지 데이터 포인트들은 결정 경계에 영향을 주지 않습니다.

3. **마진(Margin)**:
   마진은 초평면과 서포트 벡터 사이의 **거리를 최대화**하는 것이 목표입니다. SVM은 **마진이 가장 큰 초평면**을 선택하여 데이터를 구분하는데, 이 초평면을 **최대 마진 초평면(Maximum Margin Hyperplane)**이라고 부릅니다. 최대 마진을 사용하면 모델이 새로운 데이터에 더 잘 일반화됩니다.

### SVM의 작동 방식

SVM은 두 클래스 간의 **결정 경계**를 찾는 문제로, 이때 마진을 최대화하려고 합니다. 즉, **두 클래스 사이의 거리를 가능한 한 많이** 유지하면서도 데이터를 분류합니다. 이를 위해, SVM은 다음 단계를 거칩니다:

1. **초평면을 정의**: 여러 가능한 초평면 중에서 두 클래스 사이를 가장 잘 구분하는 초평면을 찾습니다.
2. **서포트 벡터 선정**: 초평면에 가장 가까운 데이터 포인트(서포트 벡터)를 찾습니다. 이 데이터 포인트들은 결정 경계에 가장 큰 영향을 미칩니다.
3. **최대 마진 초평면 선택**: 마진을 최대화하는 초평면을 선택하여 데이터를 구분합니다.

### 선형 SVM vs 비선형 SVM

#### 1. **선형 SVM**:
   선형 SVM은 **데이터가 선형적으로 분리 가능한 경우**에 사용됩니다. 이 경우, 초평면은 데이터 사이의 직선 또는 평면입니다. 선형 SVM에서는 다음과 같은 결정 초평면을 찾습니다:

   $w \cdot x + b = 0$

   여기서 $w$는 초평면의 기울기를 나타내는 벡터이고, $b$는 절편입니다.

#### 2. **비선형 SVM**:
   데이터가 선형적으로 분리되지 않는 경우에는 **비선형 SVM**을 사용합니다. 비선형 SVM은 **커널 함수(Kernel Function)**를 사용하여 데이터를 고차원 공간으로 변환한 후, 선형적으로 분리될 수 있도록 만듭니다. 이렇게 변환된 공간에서는 데이터가 선형적으로 구분될 수 있어, 선형 초평면으로도 복잡한 경계를 만들어 낼 수 있습니다.

   대표적인 커널 함수로는 다음과 같은 것들이 있습니다:
   - **폴리노미얼 커널(Polynomial Kernel)**: 다항식을 사용하여 고차원으로 변환.
   - **RBF 커널(Radial Basis Function, 가우시안 커널)**: 비선형 데이터를 고차원으로 변환하는 가장 일반적인 커널.
   - **시그모이드 커널(Sigmoid Kernel)**: 신경망의 활성화 함수와 유사한 커널.

### 소프트 마진 vs 하드 마진

1. **하드 마진(Hard Margin)**:
   하드 마진은 **오차 없이 데이터를 완벽하게 분리**하는 경우에 사용됩니다. 그러나 현실에서는 노이즈나 이상치가 있기 때문에 하드 마진은 실용적이지 않을 수 있습니다.

2. **소프트 마진(Soft Margin)**:
   소프트 마진은 약간의 **오류를 허용**하면서 데이터를 분리하는 방식입니다. 노이즈나 이상치가 있더라도 최대한 잘 분리할 수 있도록 유연성을 부여합니다. 소프트 마진에서는 **패널티**를 부여하여 데이터 포인트가 초평면의 잘못된 쪽에 위치하는 것을 최소화합니다.

### SVM의 목적 함수

SVM은 다음의 목적 함수를 통해 마진을 최대화하고 분류 오류를 최소화합니다:

$
\min \left( \frac{1}{2} ||w||^2 \right)
$

$
||w||^2
$
는 초평면의 기울기 크기를 의미합니다. 소프트 마진에서는 추가적으로 패널티 항이 포함된 목적 함수가 사용됩니다:

$
\min \left( \frac{1}{2} ||w||^2 + C \sum_{i=1}^{n} \xi_i \right)
$

여기서:
- $C$는 **패널티 항**으로, **오류 허용 정도**를 조절하는 하이퍼파라미터입니다.
- $\xi_i$는 **슬랙 변수(slack variable)**로, **서포트 벡터가 초평면의 잘못된 쪽에 위치한 정도**를 나타냅니다.

### SVM의 장점

1. **고차원 데이터에서 효율적**: SVM은 **고차원 공간**에서도 매우 효과적으로 작동합니다. 특히 데이터의 차원이 많은 경우에 강력한 성능을 발휘합니다.
2. **커널 트릭(Kernel Trick)**: SVM은 커널 함수를 사용해 비선형 데이터를 선형적으로 분리할 수 있게 만듭니다.
3. **과적합 방지**: SVM은 마진을 최대화하는 방식으로 학습하기 때문에 **과적합(overfitting)**에 강한 편입니다.
4. **명확한 결정 경계**: 마진을 최대화하는 최적의 결정 경계를 제공하여 새로운 데이터에도 잘 일반화됩니다.

### SVM의 단점

1. **대규모 데이터에 비효율적**: SVM은 **큰 데이터셋**에 대해서는 상대적으로 느리고, 메모리 요구사항이 클 수 있습니다.
2. **특성 스케일링 필요**: SVM의 성능은 **특성의 스케일(크기)**에 민감하므로, **특성 스케일링(정규화 또는 표준화)**이 필요합니다.
3. **매개변수 조정 필요**: 커널의 선택과 \( C \) 값 같은 하이퍼파라미터를 적절히 조정해야 최상의 성능을 얻을 수 있습니다.

### SVM의 사용 사례

1. **이진 분류 문제**: 스팸 필터링, 얼굴 인식, 질병 진단 등에서 SVM은 뛰어난 성능을 보입니다.
2. **이미지 분류**: 고차원 특성 공간에서 매우 효과적으로 작동하기 때문에 이미지 데이터 분류에 자주 사용됩니다.
3. **텍스트 분류**: SVM은 텍스트 분류와 같은 **고차원 희소 데이터**에서도 효과적입니다.

### 결론

SVM은 데이터를 분류할 때 마진을 최대화하여 **최적의 결정 경계**를 찾는 강력한 지도 학습 알고리즘입니다. 특히 고차원 데이터나 선형적으로 분리되지 않는 데이터에서도 좋은 성능을 발휘할 수 있도록 **커널 트릭**을 사용합니다. SVM은 분류 문제에서 매우 자주 사용되며, 복잡한 데이터 분포를 잘 처리할 수 있습니다.